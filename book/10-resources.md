# 10 — Resources & Tools  

> ⚠️ **Disclaimer:** This chapter is part of Black Hat AI and is intended for research and education only. Unauthorized testing is strictly prohibited. [Read full disclaimer →](../DISCLAIMER.md)

Where to Go When You’re Ready to Go Further

This chapter is your post-brief.  
When the drills are done and the payloads land, here’s where you sharpen the next edge.

The links below lead to public tools, labs, wargames, and deeper learning environments focused on adversarial AI, prompt hacking, and red teaming.

Use them. Fork them. Break them. Learn from them.

---

## Interactive Labs & Wargames

**[Gandalf AI Prompt Injection Challenge](https://gandalf.lakera.ai/baseline)**  
Prompt injection game that escalates in difficulty.

**[Prompting Labs (Immersive Labs)](https://prompting.ai.immersivelabs.com/)**  
Structured lab series for prompt engineering and evasion techniques.

**[DoubleSpeak](https://doublespeak.chat/#/)**  
Chat interface that shows dual responses — one filtered, one raw.  
Useful for studying moderation behavior.

**[Learn Prompting: Prompt Hacking Section](https://learnprompting.org/docs/prompt_hacking/intro)**  
Intro-level walkthrough of prompt injection, attacks, and bypasses.

**[GPT Prompt Attack (GPA)](https://gpa.43z.one/)**  
A structured platform to test prompt injection bypasses.

---

## Tools & Repos

**[AI Goat](https://github.com/dhammon/ai-goat)**  
Open-source vulnerable LLM environment designed for red teaming.

**[MyLLM Bank](https://myllmbank.com/)**  
Simulation of an AI-integrated financial assistant — with exploitable logic.

**[MyLLM Doctor](https://myllmdoc.com/)**  
AI healthcare simulation — intentionally vulnerable to adversarial prompts.

**[Spikee](https://github.com/WithSecureLabs/spikee)**  
Custom Python LLM fuzzing framework from WithSecure Labs.  
Built to probe model behavior at scale.

**[Prompt Airlines](https://promptairlines.com/)**  
A creative prompt testing environment with injection challenges.

---

## Offensive Research & Guides

**[PortSwigger: Web LLM Attacks](https://portswigger.net/web-security/llm-attacks)**  
Constantly updated collection of attack vectors targeting LLM-integrated web apps.

**[HackTheBox: AI Red Teamer Path](https://academy.hackthebox.com/path/preview/ai-red-teamer)**  
Hands-on learning path covering AI red teaming tools, techniques, and threats.

---

## Communities & Further Exploration

- [OpenAI](https://github.com/openai) – Model APIs and safety docs  
- [Hugging Face Papers](https://huggingface.co/papers) – Research feeds  
- [arXiv Computational Linguistics](https://arxiv.org/list/cs.CL/recent) – LLM security papers  
- [LLM Attacks](https://llm-attacks.org/) – Aggregated LLM vulnerabilities and demos  
- [Prompting Guide](https://promptingguide.ai) – Prompt design and misuse patterns

---

## Red Team Drill: Build Your Arsenal

1. Pick two labs from the list above  
2. Run at least one injection and one evasion attempt  
3. Log:
   - How the system reacts
   - What bypasses succeeded
   - What you learned about model behavior  
4. Add your results to your personal prompt library or payload list

Repeat weekly.

---

## Summary

This isn’t a conclusion — it’s a resupply.

You now have:
- The tactics  
- The structure  
- The mindset  
- And the field to train on

**Black Hat AI** was never meant to be the end.  
It’s the beginning of a deeper discipline.

Stay dangerous.  
Stay clean.  
Stay sharp.

[**README.**](../README.md)
