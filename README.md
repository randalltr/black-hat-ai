# Black Hat AI  
Offensive Techniques for Breaking and Bending Machine Minds

This is not a safe book.  
This is a red team field manual for exploiting AI systems ‚Äî fast, focused, and free.

**Black Hat AI** explores how large language models (LLMs) can be manipulated, hijacked, or broken through language alone.  
No reverse engineering. No shellcode. Just precision-crafted input.

---

## Read the Book

Each chapter is written in plain Markdown. Short. Tactical. Immediately actionable.

- [00 ‚Äî Foreword](00-foreword.md)  
- [01 ‚Äî Understanding the Battlefield](01-battlefield.md)  
- [02 ‚Äî Prompt Injection](02-prompt-injection.md)  
- [03 ‚Äî Jailbreaking](03-jailbreaking.md)  
- [04 ‚Äî Model Evasion](04-model-evasion.md)  
- [05 ‚Äî Embedding Attacks](05-embedding-attacks.md)  
- [06 ‚Äî Bypassing Guardrails](06-bypassing-guardrails.md)  
- [07 ‚Äî Automation & Payload Delivery](07-automation.md)  
- [08 ‚Äî Countermeasures](08-countermeasures.md)  
- [09 ‚Äî Weaponizing Your Knowledge](09-call-to-action.md)  
- [10 ‚Äî Resources & Tools](10-resources.md)

Chapters can be read in any order. New content drops incrementally.

---

## What You'll Learn

- Prompt injection, jailbreaks, and guardrail bypass  
- Input manipulation and model evasion  
- Embedding poisoning and vector search exploitation  
- How to red team AI-powered applications  
- Payload templates, tools, and working examples

This is not theory. This is live-fire practice.

---

## Usage

Clone this repository:

`git clone https://github.com/randalltr/black-hat-ai.git`


Open the chapters in any Markdown viewer (VS Code is recommended).

To export the book:

`pandoc *.md -o black-hat-ai.pdf --pdf-engine=xelatex`


For EPUB/MOBI: use Calibre or similar tools.

---

## Disclaimer

This content is for educational and research purposes only.  
Do not apply these techniques against systems you do not have explicit authorization to test.  
You are responsible for your actions.

### Extended Legal Notice

The information provided in *Black Hat AI* is intended solely for responsible research, academic study, and authorized security testing.  
By accessing or using any part of this repository, you agree to the following:

- You will **not use** the tools, code, examples, or techniques described here to attack, probe, manipulate, or disrupt any system without **explicit, written permission** from the system owner.  
- You understand that applying these methods in unauthorized environments may constitute a **violation of local, national, or international law**, including ‚Äî but not limited to ‚Äî the Computer Fraud and Abuse Act (CFAA), the DMCA, and applicable computer misuse laws in your jurisdiction.  
- The author(s) of this repository **do not condone, encourage, or support** the use of adversarial AI techniques for malicious, unethical, or exploitative purposes.

This material is provided **as-is**, without any guarantees of effectiveness, legality, or safety. The author(s) assume **no liability** for damages resulting from use or misuse, including ‚Äî but not limited to ‚Äî system downtime, data loss, account bans, employment consequences, or criminal prosecution.

If you are unsure whether your usage falls within ethical or legal bounds, **do not proceed.**

> ‚ö†Ô∏è  [Read full disclaimer ‚Üí](DISCLAIMER.md)

---

## Contributions

Created and maintained by [Randall](https://github.com/randalltr)

This project is open to forks, translations, and improvements. Pull requests are welcome.

---

## Incoming Titles

This is the first in a multi-volume set focused on AI offensive operations:

- [AI Red Teaming Playbook](https://github.com/randalltr/ai-red-teaming-playbook)  
- ~~REDACTED~~ 
- ~~REDACTED~~ 

Stay alert. Stay quiet. Stay sharp.

## License

This project is licensed under the  
**Creative Commons Attribution-NonCommercial 4.0 International License (CC BY-NC 4.0).**

You may share and adapt with attribution.  
**Commercial use is prohibited without explicit permission.**

üîó [View full license](https://creativecommons.org/licenses/by-nc/4.0/)

![License: CC BY-NC 4.0](https://img.shields.io/badge/License-CC%20BY--NC%204.0-lightgrey.svg)
